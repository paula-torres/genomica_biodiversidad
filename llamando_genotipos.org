#+AUTHOR:
#+options: ^:t f:t

* TODO Motivación
* Sub-muestreando los alineamientos
 <<submuestreo>>
** Extrayendo una región específica de los alineamientos
     <<region_especifica>>
     Recursos computacionales: 2 procesadores, 2 GB de memoria, ~10 min de
     tiempo de ejecución.
     
     En algunos casos estaremos interesados en extraer regiones específicas del
     genoma en lugar de analizarlo completamente. En nuestro curso
     particularmente queremos trabajar con algunos segmentos del genoma de
     /Heliconius/ por motivos prácticos; analizarlo todo nos tomaría mucho
     tiempo aún con pocas muestras. Además, conocemos regiones en donde hay
     genes interesantes. Para trabajar con una región específica del genoma
     aprenderemos a extraer regiones de nuestro alineamiento. Esto debemos
     hacerlo _muestra por muestra_. Usaremos ~samtools~ para esta tarea.
     Trabajaremos con el scaffold ~Hmel218003o~ de /Heliconius melpomene/ que
     contiene al gen /optix/.

     [[./Imagenes/optix_scaf.png]]

     Este gen tiene un rol clave en la adaptación de /Heliconius/ y otras
     mariposas; puedes leer más sobre él en este [[https://www.science.org/doi/pdf/10.1126/science.1208227][paper publicado en /Science/
     (2011)]], en esta [[https://www.science.org/doi/full/10.1126/science.1211025][perspectiva (2011)]] o en este [[https://www.pnas.org/content/114/40/10707][paper publicado en PNAS
     (2017).]]

     *Sigue los pasos:*
     
   1. Crea un script de ~bash~ usando ~nano~ y solicita los recursos
      necesarios. No olvides cargar el módulo de ~samtools~.

   2. Dentro del script crea un ciclo ~for~ que itere sobre los alineamientos
      ~.bam~ que ya están ordenados y sin duplicados. Si no recuerdas cómo
      escribir un ciclo ~for~ en ~bash~ puedes consultarlo aquí <INSERTAR LINK>.

   3. En cada iteración del ciclo debes llamar a ~samtools view~ con dos
      procesadores usando la opción ~-@~. Debes usar la opción ~-b~ para que el
      archivo de salida tenga formato ~bam~ y debes especificar el nombre del
      archivo de salida usando la opción ~-o~. Como argumentos para ~samtools
      view~ debes incluir primero el nombre del archivo ~bam~ original del que
      quieres extraer la región y luego debes especificar el nombre de la región
      que quieres extraer: La región que extraeremos corresponde al primer millón
      y medio de bases del scaffold ~Hmel218003o~ y la especificamos así:
      ~Hmel218003o:1-1500000~. Dale una extensión informativa a cada archivo de
      salida: Estamos extrayendo parte del scaffold ~Hmel218003o~; te sugerimos
      usar ~C18S3~ como parte del nombre de los archivos resultantes.

      La forma general de usar ~samtools view~ es:

      #+begin_src shell
        # Los argumentos dentro de los parentesis cuadrados [] son opcionales
        # Los argumentos dentro de los angulos <> son obligatorios
        samtools view [options] <in.bam>|<in.sam>|<in.cram> [region ...]
      #+end_src

   4. Dentro del ciclo ~for~, después de extraer la región de interés es
      necesario crear un índice para cada archivo de salida. Esto lo podemos
      hacer usando ~samtools index~.

      La forma general de usar ~samtools index es~:

      #+begin_src shell
        # Los argumentos dentro de los parentesis cuadrados [] son opcionales
        # los argumentos dentro de los angulos <> son obligatorios
        samtools index [-@ threads] <in.bam>
      #+end_src

   5. *Atención!* Antes de enviar el trabajo a la cola muéstrale tu script a un
      monitor/instructor para verificar que no haya errores de sintáxis.
      Muéstrale también la línea que usarás para enviar el trabajo a la cola.

   6. Envía el trabajo a la cola. Asegúrate de enviar el trabajo desde el
      directorio donde están tus archivos de entrada.
   
** COMMENT comandos
   #+begin_src shell
     # Subset bams with samtools
     # /home/juan.enciso/shared/GenomicaBiodiversidad_2021/dato_mapping/bams_subsampled_chr
     sbatch -o subset_bam.%a.out -e subset_bam.%a.err --array=1-18 subset_bams_array.sh \
            sorted_bam_list.txt Hmel218003o:1-1500000
   #+end_src
* TODO Llamando genotipos
 <<llamando_gt>>
** TODO Verosimilitudes de genotipos
** Llamada de genotipos
     <<llamada_gt>>
     Recursos computacionales: 2 procesadores, 8 GB de memoria, ~20-25 min de
     tiempo total de ejecución.

     Vamos a hacer un paso conocido como "llamada de genotipos" usando los
     alineamientos del scaffold ~Hmel218003o~ que extrajimos en la tarea
     anterior. El resultado final será un archivo en el que cada base o posición
     en el genoma aparece en una fila y cada una de nuestras muestras aparece en
     una columna.

     La estructura es similar a esta tabla, pero es un poco más compleja. Luego
     veremos en detalle la estructura de este archivo.

     |-------------+----------+------+------+----------+----------+----------+----------+-----|
     | CROMOSOMA   | POSICIÓN | REF. | ALT. | Muestra1 | Muestra2 | Muestra3 | Muestra4 | ... |
     |-------------+----------+------+------+----------+----------+----------+----------+-----|
     | Hmel218003o |        1 | A    | G    | 0/0      | 0/1      | 0/0      | 1/1      | ... |
     | Hmel218003o |        2 | T    | G    | 0/0      | 0/0      | 0/0      | 0/1      | ... |
     | Hmel218003o |        3 | G    | A    | 0/1      | 0/1      | 0/1      | 0/1      | ... |
     | Hmel218003o |        4 | C    | A    | 0/0      | 0/1      | 0/1      | 1/1      | ... |
     | Hmel218003o |        5 | C    | T    | 0/1      | 0/1      | 0/1      | 1/1      | ... |
     | Hmel218003o |        6 | T    | C    | 0/0      | 0/0      | 0/1      | 0/1      | ... |
     | ...         |      ... |      |      | ...      | ...      | ...      | ...      | ... |
     |-------------+----------+------+------+----------+----------+----------+----------+-----|
     
     Existen varias herramientas que pueden llamar genotipos, cada una tiene
     ciertos estándares y modelos probabilísticos que usa para decidir si se
     llama o no a un genotipo determinado. Las diferentes herramientas y modelos
     tienen efectos fuertes sobre los datos que se producirán, por lo que es
     importante considerar las diferentes opciones de antemano. [[https://www.nature.com/articles/nrg2986][Este review en
     Nature Genetics (2011)]] es un buen punto de partida para conocer las
     distintas perspectivas y recomendaciones al hacer este paso.

     En este caso usaremos [[https://samtools.github.io/bcftools/bcftools.html][~bcftools~]], que es parte de ~samtools~, por su
     simplicidad de uso y velocidad de ejecución. En general, independientemente
     de la herramienta utilizada este es el paso más largo de todo el proceso,
     por eso trabajaremos con una región pequeña.

   *Sigue los pasos:*
     
   1. Llamar genotipos con ~bcftools~ y otras herramientas es un proceso que da
      mejores resultados cuando se consideran los individuos _en conjunto_. Lo
      primero que debemos hacer es poner en un archivo de texto plano la lista
      de alineamientos de la región ~Hmel218003o~. Recuerda que cada
      alineamiento corresponde a un individuo; tu lista debe tener los nombres
      de los alineamientos de las 18 muestras. ¿Cómo creas este archivo con la
      lista de muestras?

   2. Crea un script de ~bash~ usando ~nano~ y solicita los recursos necesarios.
      No olvides cargar el módulo de ~samtools~.

   3. Tu script debe recibir y procesar la siguiente información (3 argumentos
      como mínimo):
      
      * El archivo con la lista de alineamientos que creaste en el paso 1

      * La ubicación del archivo ~fasta~ de referencia (ruta absoluta)

      * El nombre del archivo de salida que quieres darle a tu ~vcf~ con los
        genotipos. No olvides que vamos a escribir un ~vcf~ comprimido
        (~.vcf.gz~).

   4. Dentro de este script haremos tres pasos: _El primer paso_ consiste en
      calcular las verosimilitudes de los genotipos a partir de los datos
      observados en el alineamiento. Para este paso usamos [[https://samtools.github.io/bcftools/bcftools.html#mpileup][~bcftools mpileup~]].
      Lee la descripción de las opciones de este programa en el enlace. Las
      opciones relevantes para nuestro análisis son: ~-O~, ~--threads~,
      ~--max-depth~, ~-q~, ~-Q~, ~-P~, ~-a~, ~-f~ y ~-b~. Usa una profundidad
      máxima de 1000, umbrales de calidad de base y de mapeo de 20 y especifica
      que nuestros datos se produjeron en una plataforma Illumina. Dedícale un
      poco más de atención a la opción ~-a~, que se usa para especificar con qué
      información estará anotado nuestro archivo ~vcf~. Queremos incluir la
      profundidad alélica (~AP~) y el número de bases de alta calidad que dan
      soporte a cada sitio (~DP~). Revisa el manual de ~bcftools mpileup~
      (opción ~-a~) para conocer cómo especificar estas dos anotaciones.

      La forma general de usar ~bcftools mpileup~ es:

      #+begin_src shell
        # Llamaremos de esta forma bcftools mpileup
        # Resaltamos dos piezas de informacion importantes:
        # La referencia y la lista de alineamientos
        bcftools mpileup [opciones] -f ref.fa -b lista_de_bams.txt
      #+end_src

      La salida de ~bcftools mpileup~ la re-dirigiremos hacia ~bcftools call~
      usando el operador "pipe" de unix. Recuerdas cómo usar este operador?

   5. _El segundo paso_ utiliza las verosimilitudes calculadas por la
      herramienta ~mpileup~ y las bases observadas en cada posición del
      alineamiento para determinar el genotipo de cada individuo en esa
      posición. El resultado conjunto de estos dos primeros pasos se escribe en
      un archivo ~vcf~ comprimido con [[https://www.htslib.org/doc/bgzip.html][~bgzip~]] (extensión ~.vcf.gz~). Utilizamos
      [[https://samtools.github.io/bcftools/bcftools.html#call][~bcftools call~]] para este propósito. Lee la descripción de las opciones de
      este programa en el enlace. Las opciones relevantes para nuestro análisis
      son: ~-m~, ~--threads~, ~-f~, ~-O~ y ~-o~. Utiliza el "multiallelic
      caller", dos procesadores, calidad del genotipo (GQ) en el campo de
      formato para cada muestra y vcf comprimido como tipo de archivo de salida.
      Dale un nombre informativo a tu archivo de salida.

      La forma general de usar ~bcftools call~ es:
   
      #+begin_src shell
        # No olvides conectar la salida estandar de mpileup
        # con este comando
        bcftools call [opciones] -o genotipos.vcf.gz
      #+end_src

   6. Finalmente en _el tercer paso_ generamos un índice del archivo ~vcf.gz~
      para hacer operaciones de forma más rápida con él. Utiliza [[https://samtools.github.io/bcftools/bcftools.html#index][~bcftools
      index~]] con dos procesadores para construir este índice. Si se ejecuta
      correctamente debería crearse un archivo con un nombre idéntico al creado
      en el paso anterior pero con la extensión ~.csi~ añadida al final; este
      archivo es el índice.

      La forma general de usar ~bcftools index~ es:
   
      #+begin_src shell
        # Sintaxis para construir un indice con bcftools
        # como archivo de entrada puedes tener formatos vcf o bcf
        bcftools index [opciones] genotipos.vcf.gz|genotipos.bcf
      #+end_src

   7. *Atención!* Antes de enviar el trabajo a la cola muéstrale tu script a un
      monitor/instructor para verificar que no haya errores de sintáxis.
      Muéstrale también la línea que usarás para enviar el trabajo a la cola.

   8. Envía el trabajo a la cola. Asegúrate de enviar el trabajo desde el
      directorio donde están tus archivos de entrada.

*** COMMENT Respuesta script
    #+begin_src shell
      #!/bin/bash
      #SBATCH -p normal
      #SBATCH -n 2
      #SBATCH --mem=16000
      #SBATCH --time=3-12:00
      
      module load samtools
      
      bamlist=$1
      refpath=$2
      bamfolder=$3
      outdir=$4
      outfile=$5
      
      echo $(date)
      
      cd $bamfolder
      
      bcftools mpileup -Ou --threads 2 \
               --max-depth 10000 \
               -q 20 -Q 20 -P Illumina \
               -a FORMAT/DP,FORMAT/AD \
               -f $refpath -b $bamlist | \
          bcftools call -m --threads 2 \
                   -f GQ \
                   -O z \
                   -o $outdir/$outfile
      
      bcftools index $outdir/$outfile
    #+end_src
      
** COMMENT Comandos
   #+begin_src shell
     # Call genotypes using bcftools
     # $1 bamlist $2 refpath $3 bamsfolder $4 outdir
     # STARTED
     sbatch -o genotyping.out -e genotyping.err call_GT_bcftools.sh bamlist_to_call.txt \
            /datacnmat01/biologia/biologia.evolutiva/shared/Hmel2.5/Hmel2.5_with_mtDNA.fa \
            . . heliconius.GT.vcf.gz
     
     # Calling only a small region Hmel218003o:1-1500000
     sbatch -o genotyping.small.out -e genotyping.small.err call_GT_bcftools.sh \
            smallr_bams_list.txt \
            /datacnmat01/biologia/biologia.evolutiva/shared/Hmel2.5/Hmel2.5_with_mtDNA.fa \
            . . heliconius.optixscaf.GT.ALLSITES.vcf.gz    
     # job 152995
   #+end_src

* TODO Operaciones con archivos VCF/BCF
 <<operaciones_vcf>>
** TODO El formato VCF
   <<formato_vcf>>
   Descripción del formato VCF.
   
** Extrayendo información
   <<extrayendo_info_vcf>>
   Es posible utilizar las herramientas de unix que hemos aprendido a usar para
   explorar la información de un archivo ~vcf~. Podemos incluso explorar nuestro
   archivo sin necesidad de descomprimirlo (recuerda que es un vcf comprimido).
   Explora el archivo usando las herramientas que consideres necesarias y trata
   de *no* descomprimirlo usando ~gunzip~.

   *Responde a las preguntas:*
   
    1. Cuántas líneas tiene mi archivo de genotipos en total?
    2. Cuántas líneas tiene el encabezado?
    3. Cuántas líneas tiene el cuerpo principal?
    4. Si la región que extrajimos del scaffold ~Hmel218003o~ tiene 1500000
       pares de bases, por qué el cuerpo principal tiene más posiciones?
    5. Todas las posiciones que extrajimos del scaffold ~Hmel218003o~ están
       representadas?

*** COMMENT Respuestas:
    1. 1502824
    2. 363
    3. 1502461
    4. Cuando hay indels la posición se cuenta dos veces
    5. No, por ejemplo las posiciones 1 a 132 no aparecen
    
** Tipos de variantes
   <<tipos_variantes>>
   Cuando genotipificamos podemos encontrar esencialmente tres tipos de sitios:
   Sitios sin variación con respecto a la referencia, sitios con variación
   simple (SNPs) o sitios que potencialmente pueden tener mutaciones
   estructurales como inserciones o deleciones (INDELS). Exploremos nuestro
   archivo para tener una mejor idea de cómo pueden verse estas variantes.
   
    *Responde a las preguntas:*
    
    1. Qué tipo de variante observamos en la posición 281946 del scaffold ~Hmel218003o~?
    2. Cuál es el alelo encontrado en la referencia?
    3. Cuál es el alelo alternativo encontrado en algunas de las muestras?
    4. Cuáles muestras tienen el alelo alternativo en esta posición?
    5. Cuántas muestras no tienen genotipo en esta posición?
    6. Cuántos indels hay en el archivo de genotipos?
    7. Cómo harías para encontrar qué sitios tienen SNPs y qué sitios son
       invariantes?
       
*** COMMENT Respuestas:
    Hmel218003o	281946	.	AATAGCCCAT	AAT
    1. INDEL
    2. AATAGCCCAT
    3. AAT
    4. H.par.spn.JM371 (heterocigota)
    5. 6 de las 18 muestras
    6. 35285
       
** TODO Filtrando sitios
   <<filtrando_sitios>>
   Recursos computacionales: 2 procesadores, 2 GB de memoria, ~30 min de tiempo
   total de ejecución.
   
   Usaremos [[https://vcftools.github.io/man_latest.html][~vcftools~]] para filtrar. Existen otras herramientas disponibles para
   hacer esto, como ~bcftools~ o ~GATK~. ~vcftools~ es razonablemente simple y
   nos permite calcular algunas estadísticas sobre nuestras muestras para
   decidir qué filtros aplicar. Podemos visualizar estas estadísticas en ~R~
   para facilitar el análisis.
   
*** Calculando estadísticas en nuestros genotipos
    <<calcula_estadisticas>>
    1. *Preparando el análisis por sitios:* Es necesario modificar nuestro
       archivo de genotipos para poder aplicar filtros correctamente y hacer
       análisis posteriores de forma correcta; la mayoría de análisis y modelos
       en genética de poblaciones están diseñados considerando individualmente
       sitios con dos alelos (bialélicos). Debemos entonces quitar los indels y
       los sitios con más de dos alelos de nuestro archivo ~vcf~. Para quitar
       los indels usamos la opción ~--remove-indels~. Para quedarnos con sitios
       con uno o dos alelos usamos la opción ~--max-alleles~. La opción
       ~--recode~ se usa para tener información de salida en formato ~vcf~. La
       opción ~--recode-INFO-all~ se usa para mantener el encabezado del vcf
       original y la opción ~--out~ se usa para darle un pre-fijo al nombre del
       archivo de salida. ~vcftools~ escribe un archivo sin comprimir, cuando
       tengas el resultado en un vcf comprímelo con ~bgzip~.

       La forma general de usar ~vcftools~ es la siguiente:
    
       #+begin_src shell
         # Consulta los ejemplos en el manual de vcftools para que
         # tengas mas claridad de la sintaxis que usaras
         vcftools [--vcf ARCHIVO | --gzvcf ARCHIVO | --bcf ARCHIVO] \
                  [--out PREFIJO] [OPCIONES DE FILTRO] [OPCIONES DE SALIDA]
       #+end_src

       Puedes correr ~vcftools~ en una sesión interactiva de slurm, recuerda
       solicitar la sesión interactiva con ~salloc~.

       *Atención:* Muéstrale tus líneas de comando a un monitor o instructor
       antes de correrlas.

       Si corriste ~vcftools~ correctamente debes observar algo parecido a esto:

       #+begin_src shell
         VCFtools - 0.1.16
         (C) Adam Auton and Anthony Marcketta 2009
         
         Parameters as interpreted:
         ...
         
         Using zlib version: 1.2.11
         Warning: Expected at least 2 parts in INFO entry: ...
         Warning: Expected at least 2 parts in INFO entry: ...
         Warning: Expected at least 2 parts in INFO entry: ...
         After filtering, kept 18 out of 18 Individuals
         Outputting VCF file...
         After filtering, kept 1454897 out of a possible 1502460 Sites
         Run Time = 31.00 seconds
       #+end_src

       Para poder hacer algunos de los filtros que haremos en esta práctica es
       necesario agregar una pieza de información que no está aún en nuestro
       vcf: El identificador de cada sitio. Fíjate en la tercera columna del
       archivo de genotipos; tiene puntos que representan ausencia de
       información.

       #+begin_src shell
         #CHROM  POS     ID      REF     ALT
         Hmel218003o     133     .       G       .
         Hmel218003o     134     .       T       .
         Hmel218003o     135     .       A       .
         Hmel218003o     136     .       T       .
         Hmel218003o     137     .       G       .
         Hmel218003o     138     .       T       .
       #+end_src

       Necesitamos convertir estos puntos en identificadores únicos de cada
       sitio. Una solución usada a menudo para nombrar los sitios es el formato
       ~scaffold:posicion~. Para hacer esto con nuestros datos vamos a usar la
       herramienta [[https://www.gnu.org/software/sed/][~sed~]] (<b>s</b>tream <b>ed</b>itor). Haremos varios pasos
       para incluir esta información en nuestro archivo. El paso _clave_ utiliza
       ~sed~ para sustituir todo lo que empiece con ~Hmel~ seguido de cualquier
       cantidad de dígitos, seguido de una o minúscula (primera columna), una
       tabulación, varios dígitos seguidos (segunda columna), otra tabulación y
       un punto por la primera columna, una tabulación, la segunda columna, otra
       tabulación, y las columnas 1 y 2 unidas por el caracter ~:~. Debemos
       volver a comprimir usando [[https://www.htslib.org/doc/bgzip.html][~bgzip~]], enviando el resultado a la salida
       estándar y luego re-dirigimos la salida estándar al nombre que queremos
       darle a los datos usando el operador ~>~. Finalizamos indexando este
       nuevo archivo con ~bcftools index~. Los procesos de descomprimir,
       sustituir y comprimir nuevamente están unidos entre sí por el operador
       ~|~. Lee con atención los siguientes comandos.
    
       #+begin_src shell
         # Cambiamos el nombre de nuestro archivo a un nombre transitorio
         mv heliconius.optixscaf.SNPS.NV.vcf.gz heliconius.optixscaf.SNPS.NV.NOID.vcf.gz
         
         # Descomprimimos y agregamos los id por sitio
         # comprimimos con bgzip y enviamos a la salida estandar
         # re-dirigimos al nombre de archivo original
         zcat heliconius.optixscaf.SNPS.NV.NOID.vcf.gz \
             | sed -e 's/\(Hmel[[:digit:]]\+o\)\t\([[:digit:]]\+\)\t\./\1\t\2\t\1:\2/g' \
             | bgzip -c > heliconius.optixscaf.SNPS.NV.vcf.gz
         
         # indexamos nuevamente con bcftools
         bcftools index heliconius.optixscaf.SNPS.NV.vcf.gz
       #+end_src

       Listo! Después de esta operación complicada tenemos sitios con
       identificador; revisa el contenido del nuevo archivo con ~zless -s~.
       Nota el cambio.

       #+begin_src shell
         #CHROM  POS     ID      REF     ALT
         Hmel218003o     133     Hmel218003o:133 G       .
         Hmel218003o     134     Hmel218003o:134 T       .
         Hmel218003o     135     Hmel218003o:135 A       .
         Hmel218003o     136     Hmel218003o:136 T       .
         Hmel218003o     137     Hmel218003o:137 G       .
         Hmel218003o     138     Hmel218003o:138 T       .
       #+end_src

       Podemos continuar con los siguientes pasos.
    
    2. *Contando alelos:*

       El primer criterio que usaremos para filtrar algunos sitios es el número
       de alelos que observamos en ellos. Si el número de individuos con alelos
       en un sitio es muy bajo, y si hay alelos "raros" en un sitio, podríamos
       tener efectos negativos en nuestros análisis posteriores. En parte esto
       se debe a que muchos de los modelos y análisis disponibles son sensibles
       a la presencia de variantes raras (presentes en baja frecuencia).

       Usaremos nuestro archivo con sitios invariantes y bi-alélicos para
       calcular los conteos de alelos por sitio. Llama a ~vcftools~ usando la
       opción ~--counts2~ para contar los alelos por sitio. No olvides
       especificar un prefijo para el nombre de salida (~--out~). El archivo de
       salida debe tener la extensión ~.frq.count~ (~vcftools~ pone la extensión
       automáticamente).

       Abre el archivo resultante usando ~less~, deberías ver algo como esto:

       #+begin_src shell
         CHROM   POS     N_ALLELES       N_CHR   {COUNT}
         Hmel218003o     133     1       0       0
         Hmel218003o     134     1       2       2
         Hmel218003o     135     1       2       2
         Hmel218003o     136     1       2       2
         Hmel218003o     137     1       2       2
       #+end_src

       Si te fijas, algunas filas tienen 6 elementos y otras tienen 5 ¿Por qué
       pasa esto? Revisa atentamente el archivo y trata de responder a la
       pregunta.

       Para trabajar con un archivo como este en ~R~ es más sencillo tener el
       mismo número de columnas en todas las filas. Las filas con 5 columnas
       solo tienen el conteo del alelo de referencia y no del alelo alterno, ese
       conteo es de 0 (¿Por qué?) y debemos agregarlo por nuestra cuenta.

       Usaremos el lenguaje [[https://www.gnu.org/software/gawk/manual/gawk.html][~awk~]] para hacer esta tarea. ~awk~ procesa el
       archivo línea por línea y permite explorar varias propiedades de cada
       línea. El razonamiento es el siguiente: Si una línea tiene menos de 6
       columnas (~NF~) entonces debemos imprimir la línea original ($0) y
       adjuntar un 0 al final (el número de alelos alternos en el sitio),
       separado por un caracter de tabulación ~"\t"~. En caso contrario
       imprimimos la línea original (~$0~).

       El razonamiento anterior se captura en la siguiente línea de comando
       usando ~awk~. Asegúrate de que entiendes la línea antes de ejecutarla. Si
       tienes dudas pide aclaraciones al personal docente o a tus compañeros de
       curso.

       #+begin_src shell
         # Sintaxis:
         # awk 'codigo de awk' archivo
         awk '{if(NF < 6){print $0 "\t" 0} else {print $0}}' archivo.conteos
       #+end_src

       Reemplaza el nombre ~archivo.conteos~ por el nombre de tu archivo. El
       resultado de la operación va a la salida estandar, re-dirígelo a un nuevo
       archivo con un nombre informativo; en mi caso el nuevo archivo se llama
       ~heliconius.optixscaf.conteofull~.

       Finalmente debemos editar el encabezado para que sea leido correctamente
       por ~R~. Abre el archivo con ~nano~ y cambia el nombre de la columna
       ~{COUNT}~ por ~CONTEO_REF~ y el ~0~ que aparece como nombre de la última
       columna por ~CONTEO_ALT~.

       _Antes:_
       #+begin_src shell
         CHROM	POS	N_ALLELES	N_CHR	{COUNT}	0
         Hmel218003o	133	1	0	0	0
         Hmel218003o	134	1	2	2	0
         Hmel218003o	135	1	2	2	0
       #+end_src

       _Después:_
       #+begin_src shell
         CHROM	POS	N_ALLELES	N_CHR	CONTEO_REF	CONTEO_ALT
         Hmel218003o	133	1	0	0	0
         Hmel218003o	134	1	2	2	0
         Hmel218003o	135	1	2	2	0
       #+end_src
       
    3. *Calculando profundidad promedio de secuenciación por individuo:*

       La profundidad de sencuenciación es importante pues ayuda a informar los
       soportes estadísticos para llamar determinados alelos. En general, se
       considera que los datos soportados por profundidades bajas tienen un
       mayor nivel de incertidumbre que aquellos en donde la profundidad es
       mayor.

       A nivel de individuo la profundidad también tiene un efecto sobre la
       calidad de los análisis: Si un individuo tiene profundidad muy baja a lo
       largo de todo su genoma puede afectar las estadísticas de todo el set de
       datos y sería preferible excluirlo.

       Llama a ~vcftools~ usando la opción ~--depth~ para calcular la
       profundidad promedio por individuo. No olvides especificar un prefijo
       para el nombre de salida (~--out~). El archivo de salida debe tener la
       extensión ~.idepth~. Para estos datos debemos ver valores entre 18 y 37
       de profundidad aproximadamente.
    
    4. *Calculando profundidad promedio de secuenciación por sitio:*

       A nivel de sitio el efecto de la profundidad baja no es muy diferente: Si
       un sitio tiene baja profundidad de cobertura es más difícil confiar en
       los alelos presentes en ese sitio. También es recomendable remover sitios
       con baja profundidad de secuenciación.

       Calculamos la profundidad por sitio usando la opción ~--site-mean-depth~.
       El archivo de salida debe tener la extensión ~.ldepth.mean~.
       
    5. *Calculando calidad de alineamiento por sitio (~QUAL~):*

       La calidad de alineamiento por sitio nos dice qué tan bien alineadas
       están las lecturas que cubren una región determinada.

       Calculamos la calidad de alineamiento por sitio usando la opción
       ~--site-quality~. El archivo de salida debe tener la extensión ~.lqual~.
       
    6. *Calculando la proporción de datos perdidos por individuo:*

       Los sitios con datos perdidos son aquellos en donde no hubo evidencia
       suficiente para llamar un genotipo durante el paso de llamada. Estos
       sitios aparecen en el archivo ~vcf~ como ~./.~, puedes dar un ejemplo de
       un sitio en algún indivuduo que no tenga genotipo llamado?

       Los individuos con una proporción grande de sitios perdidos pueden causar
       problemas en los análisis; de ser tenidos en cuenta sería necesario
       eliminar muchos sitios potencialmente informativos para mantener la
       calidad de los datos en general.

       Calculamos la proporción de datos perdidos por individuo usando la opción
       ~--missing-indv~. La extensión del archivo de salida debe ser ~.imiss~.

       El archivo de salida es pequeño y puedes explorarlo. Responde: ¿Qué
       individuos tienen las tasas más altas de datos perdidos? ¿Notas algún
       patrón? ¿Cuál puede ser la razón biológica para estas observaciones?
    
    7. *Calculando la proporción de datos perdidos por sitio:*

       La proporción de datos perdidos por sitio nos permite determinar regiones
       del genoma que fueron difíciles de alinear para la mayoría de los
       individuos y que por lo tanto no serán informativas. En cierta forma esto
       ya lo tenemos en cuenta cuando contamos el número de alelos por sitio.
       Tenemos 18 individuos dipliodes; un sitio con cantidad perfecta de
       información tendrá entonces 36 alelos.

       Calculamos la proporción de datos perdidos por sitio usando la opción
       ~--missing-site~. La extensión del archivo de salida debe ser ~.lmiss~.
       Ya hicimos un paso equivalente a este en el paso 2 de esta sección
       (contando alelos). En este punto el cálculo sería redundante.

    8. *Transfiriendo los datos a nuestra máquina*

       Finalmente copia a tu máquina los archivos creados usando ~scp~ o
       ~rsync~.
    
**** COMMENT Respuestas

     1. Preparando los archivos
        #+begin_src shell
          # Solo sitios invariantes y bialelicos
          vcftools --gzvcf heliconius.optixscaf.GT.ALLSITES.vcf.gz  \
                   --remove-indels --max-alleles 2 --recode \
                   --recode-INFO-all --out heliconius.optixscaf.SNPS.NV
          
          mv heliconius.optixscaf.SNPS.NV.recode.vcf heliconius.optixscaf.SNPS.NV.vcf
          
          bgzip heliconius.optixscaf.SNPS.NV.vcf
       #+end_src

     2. Contando alelos
        #+begin_src shell
          vcftools --gzvcf heliconius.optixscaf.SNPS.NV.vcf.gz --counts2 \
                   --out heliconius.optixscaf.2
          
          awk '{if(NF < 6){print $0 "\t" 0}else{print $0}}' \
              archivo.conteos > archivo.conteos.full
        #+end_src

     3. Calculando profundidad x individuo
        #+begin_src shell
         vcftools vcftools --gzvcf heliconius.optixscaf.SNPS.NV.vcf.gz --depth \
                  --out heliconius.optixscaf.2
        #+end_src
       
     4. Calculando profundidad promedio x sitio
        #+begin_src shell
         vcftools --gzvcf heliconius.optixscaf.SNPS.NV.vcf.gz --site-mean-depth \
                  --out heliconius.optixscaf.2
        #+end_src
       
     5. Calculando calidad de inferencia por sitio
        #+begin_src shell
         vcftools --gzvcf heliconius.optixscaf.SNPS.NV.vcf.gz --site- \
                  --out heliconius.optixscaf.2
        #+end_src

     6. Calculando la proporción de datos perdidos por individuo
        #+begin_src shell
         vcftools --gzvcf heliconius.optixscaf.SNPS.NV.vcf.gz --missing-indv \
                  --out heliconius.optixscaf.2
        #+end_src

     7. Calculando la proporción de datos perdidos por sitio
        #+begin_src shell
         vcftools --gzvcf heliconius.optixscaf.SNPS.NV.vcf.gz --missing-site \
                  --out heliconius.optixscaf.2
        #+end_src
     
*** Analizando y visualizando las estadísticas de los genotipos en ~R~
    1. *Preparando el ambiente de trabajo*

       Debemos asegurarnos de que tenemos nuestro ~R~ en el estado apropiado
       antes de trabajar analizando los datos. Empecemos creando un nuevo script
       en ~Rstudio~ y dándole un nombre apropiado.

       #+begin_src r
         ### Empezamos limpiando el ambiente de trabajo
         rm(list = ls())
         
         ### cambia la ruta actual de trabajo a la carpeta donde
         ### guardas los datos generados en la sección anterior
         setwd("~/ruta/de/trabajo")
       #+end_src

       Si no lo tenemos instalado, instalamos el paquete [[https://www.tidyverse.org/][~tidyverse~]], que es una
       colección de paquetes diseñada para trabajar en análisis de datos.
    
       #+begin_src r
         ### Si no tenemos instalado tidyverse, ejecutamos
         install.packages("tidyverse")
         
         ### Cargamos el paquete
         library(tidyverse)
       #+end_src

    2. *Estadísticas por sitio: Conteo de alelos*
       #+begin_src r
         ### Cargamos los datos de conteo de alelos
         conteo_alelos <- read_tsv("archivo.conteo")
       #+end_src

       Debemos tomar decisiones sobre el número de alelos que queremos como
       representación mínima para un sitio. No tenemos muchas muestras, apenas
       18 individuos dipliodes, entonces idealmente en todos los sitios
       deberíamos tener 36 alelos. Si examinamos en detalle los datos nos damos
       cuenta que muchos sitios están lejos de tener los 36 alelos. ¿Cómo
       podemos ver un resumen de lo que está pasando con nuestros datos? Podemos
       revisarlo con la función ~summary~ de ~R~. También podemos visualizarlo:
       Hagamos un histograma de los conteos de alelos por sitio usando las
       funciones del paquete ~ggplot2~.

       #+begin_src r
         ### Pintamos un histograma de los conteos de alelos por sitio
         ggplot(data=conteo_alelos, aes(x=N_CHR)) + geom_histogram() +
           labs(x="Num. alelos", y="Conteo")
       #+end_src

       El resultado debe verse así:
       [[./Imagenes/conteo_alelos_x_sitio.png]]

       ¿Cómo podemos usar la información que acabamos de graficar para tomar
       decisiones? Podemos ver en la gráfica que la mayoría de sitios tienen 30
       o más alelos, lo cual corresponde a tener información aproximadamente
       para el 83% de los individuos. Este número es razonable, tendríamos
       relativamente buena información en los sitios que tengan 30 o más alelos.
       Vamos a conservar los sitios con 30 o más alelos.

    3. *Estadísticas por sitio: Profundidad promedio*

       Normalmente en un experimento de secuenciación la profundidad es
       altamente variable; algunos sitios no tienen cobertura mientras que otros
       están cubiertos por cientos o miles de lecturas. Debemos examinar la
       variación de profundidad en los sitios que estamos analizando. Para esto
       cargamos y graficamos los datos de la siguiente forma:
       
       #+begin_src r
         ### Cargamos los datos de profundidad promedio por sitio
         prof_avg_sitio <- read_tsv("heliconius.optixscaf.2.ldepth.mean")
         
         ### Pintamos un histograma de la profundidad por sitio
         ggplot(data=prof_avg_sitio, aes(x=MEAN_DEPTH)) + geom_histogram() +
           labs(x="Prof. prom. x sitio", y="Conteo")
       #+end_src

       El resultado debe verse así:

       [[./Imagenes/prof_promedio_sitio.png]]

       Preguntas: ¿Cuál es el rango principal de distribución de los datos de
       profundidad? ¿Por qué el eje x tiene su límite derecho tan lejos?

       Normalmente se considera que en un experimento de secuenciación la
       profundidad debe ser mayor a 5 (5 lecturas soportando los alelos
       observados en una posición, a menos que el experimento haya sido diseñado
       desde el principio con baja profundidad). Según nuestra gráfica es raro
       observar sitios con profundidad de secuenciación de 50 o más; las
       profundidades muy altas pueden ser resultado de artefactos y es razonable
       excluirlas. Por lo tanto, vamos a incluir sitios cuyas profundidades
       promedio estén en el intervalo $5 \leq profundidad \leq 50$.

    4. *Estadísticas por individuo: Profundidad promedio*

       Los datos de estadísticas por individuo no son muy grandes entonces
       podemos verlos sin necesidad de una gráfica. Sin embargo, vamos a
       graficarlos para practicar nuestras habilidades de presentación de datos
       con ~R~. Exploremos la profundidad promedio por indivuduo.
       
       #+begin_src r
         ### Profundidad promedio por individuo
         prof_prom_indv <- read_tsv("heliconius.optixscaf.2.idepth")
         
         ### Graficamos los datos
         ggplot(data=prof_prom_indv, aes(x=INDV, y=MEAN_DEPTH)) + geom_point() +
           labs(x="Individuo", y="Prof. promedio") + 
           theme(axis.text.x = element_text(angle = 45, hjust=1))
       #+end_src

       [[./Imagenes/prof_prom_indv.png]]

       Podemos observar que nuestros individuos tienen todos profundidad
       promedio superior a 20, lo cual se considera bueno. Si tuvieramos
       individuos cuyos promedios de profundidad están por debajo de 3 podríamos
       considerar excluirlos del análisis.

    5. *Estadísticas por individuo: Datos perdidos*

       Exploremos el porcentaje de datos perdidos que cada individuo tiene en la
       region ~Hmel218003o:1-1500000~.
    
       #+begin_src r
         ### Estadisticas por individuo
         ### Importando datos
         datos_perdidos_indv <- read_tsv("heliconius.optixscaf.2.imiss")
         
         ### Pintando la grafica
         ggplot(data=datos_perdidos_sitio, aes(x=INDV, y=F_MISS)) + geom_point() +
           labs(x="Individuo", y="% Datos perdidos") + 
           theme(axis.text.x = element_text(angle = 45, hjust=1))
       #+end_src

       [[./Imagenes/datos_perdidos_indv.png]]

       Pregunta: ¿Observando la información de los datos perdidos, crees que hay
       algún individuo que deba excluirse?

*** TODO Aplicando los filtros al archivo VCF
    1. *Criterio basado en conteos:*

       Este es el filtro más complejo de los tres. Lo que debemos hacer en este
       caso es anlizar nuestros datos de conteo de alelos en ~R~, seleccionar
       los sitios que cumplen con el criterio que establecemos y exportar estos
       datos en el formato requerido.

       Recuerda que en la sección donde [[calcula_estadisticas][preparábamos el análisis por sitios]],
       hicimos una operación en la que le dábamos a cada sitio un identificador
       único.
       
       #+begin_src r
         ### Sitios con mas de 30 alelos en los cuales
         ### la frecuencia del alelo menor debe ser mayor que 0.1
         ### creamos una nueva columna llamada ID_SNP
         ### con el identificador de los sitios que vamos a conservar
         ### seleccionamos solo esta columna y asignamos el nombre sitios_cont
         sitios_cont <- conteo_alelos %>% filter(N_CHR >= 30) %>%
           mutate(AF=ifelse(CONTEO_ALT < CONTEO_REF, CONTEO_ALT/N_CHR,
                            CONTEO_REF/N_CHR)) %>%
           filter(AF == 0 | AF > 0.1) %>%
           mutate(ID_SNP=paste(CHROM, POS, sep=":")) %>%
           select(ID_SNP)
         
         ### Escribimos estos datos en un archivo ignorando el encabezado
         write_tsv(sitios_cont, "sitios_retenidos_cont.txt",
                   col_names = FALSE)
       #+end_src

       Aplicamos el filtro: Le decimos a ~vcftools~ que debe dejar únicamente
       estos sitios y descartar el resto. Usamos la opción ~--snps~.
    
       #+begin_src shell
         vcftools --gzvcf heliconius.optixscaf.SNPS.NV.vcf.gz \
                  --snps sitios_retenidos_cont.txt --recode --recode-INFO-all \
                  --stdout | bgzip -c > heliconius.optixscaf.SNPS.NV.FL1.vcf.gz
         
         # After filtering, kept 1067052 out of a possible 1454897 Sites
         bcftools index heliconius.optixscaf.SNPS.NV.FL1.vcf.gz
       #+end_src
       
    2. *Criterio basado en profundidad:*
    3. *Criterio basado en datos perdidos:*



    
**** COMMENT Respuestas

     1. Filtro 1
        #+begin_src shell
         vcftools --gzvcf heliconius.optixscaf.SNPS.NV.vcf.gz \
                  --snps sitios_retenidos_cont.txt --recode --recode-INFO-all \
                  --out heliconius.optixscaf.SNPS.NV.FL1
         
         # After filtering, kept 1067052 out of a possible 1454897 Sites
         
         mv heliconius.optixscaf.SNPS.NV.FL1.recode.vcf \
            heliconius.optixscaf.SNPS.NV.FL1.vcf
         
         bgzip heliconius.optixscaf.SNPS.NV.FL1.vcf
         
         bcftools index heliconius.optixscaf.SNPS.NV.FL1.vcf.gz
        #+end_src
     2.  
     3. 
